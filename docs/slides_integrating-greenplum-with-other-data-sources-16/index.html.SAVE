<!DOCTYPE html>
	<html class="sl-root decks export offline loaded">
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
		<title>Integrating Greenplum With Other Data Sources</title>

		<meta name="description" content="Integrating Greenplum With Other Data Sources">

		<link rel="stylesheet" type="text/css" href="lib/offline-v2.css">



	</head>
	<body class="reveal-viewport theme-font-montserrat theme-color-white-blue">
		<div class="reveal">
			<div class="slides">
				<section data-background-image="integrating-greenplum-with-other-data-sources-16/892ed9eecaaf4c43e2445fa6c9ed0e39.png" data-background-size="contain" data-id="ae546e6021aec2041ef2641f0cfe5292"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/27cd19956e071008c873fe7af5631637.png" data-background-size="contain" data-id="ebab84e1844796e4146cb89d057c3523"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/48572d8f7ea3f2713280defcd84b3e23.png" data-background-size="contain" data-id="8cff6eb731897a85114cf1b366ae5824"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/d241cbd026204f868ec94e7ba1f97ab3.png" data-background-size="contain" data-id="45afadb5ca148c10bfed7dce92e70d12"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/80ce1a0fa7f595edfbf0c91237d07660.png" data-background-size="contain" data-id="4599448d37aafa870c97fb7044eea99d"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/6493d0536698077021ecd432ea66715e.png" data-background-size="contain" data-id="3e8d0a65f010c952e92ece2b88605532"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/01a22ebfc71351f24f92ec2effa67c9d.png" data-background-size="contain" data-id="8fb8479ef04d73d448b78bb37736f3e0"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/85f8c0e8af00b1fa1798e1e45aef4c74.png" data-background-size="contain" data-id="5b64c01e6059dd65c69e8810f31f9e30"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/4323e6b908cdf57a5fe5a291c5f45cce.png" data-background-size="contain" data-id="3ea7fdf503cd249eb4a3c0ac969ccb5c"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/90a066382ed0b7ad64844973dba00c1b.png" data-background-size="contain" data-id="54e151cd4a6bc0ff854f8236b60adcb0"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/6311c07be8a6e72429e00236d63b5aff.png" data-background-size="contain" data-id="48cf18c4f7c3d41109bacf9b4b5599d1"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/57295037e562541d5845314c7221651d.png" data-background-size="contain" data-id="738b238ee0153fb247f3c9e907212bd9"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/ec3593564f3a063aa2577ee3af9048ff.png" data-background-size="contain" data-id="0e5fe2230b3716e582d93ddd5893754d"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/5fa3629fc318da2d8ad162842971af99.png" data-background-size="contain" data-id="f3ddf7d191412b99207d9ef8d403a0c4"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/fa002b20ae7a840a26210ce70f14d583.png" data-background-size="contain" data-id="473b817a84cae39ed577045bb9ad27c2"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/a13fe697a6813c3552eba4c8e5c50322.png" data-background-size="contain" data-id="c191ffd2704703c21028ce3aba469e24"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/89d8659eea4f0797f62e128c9b797476.png" data-background-size="contain" data-id="9e92bc3c259411cb0bcd43eb33d71e18"></section><section data-background-image="integrating-greenplum-with-other-data-sources-16/f505ac680a12a351878255cd9a1d9096.png" data-background-size="contain" data-id="279bf5d611b67eb0271ac02268b27b19"></section>
			</div>
		</div>

		<script>
			var SLConfig = {"deck": {"id":1582004,"slug":"integrating-greenplum-with-other-data-sources-16","title":"Integrating Greenplum With Other Data Sources","description":"Integrating Greenplum With Other Data Sources","width":960,"height":700,"visibility":"self","published_at":null,"sanitize_messages":null,"thumbnail_url":"https://s3.amazonaws.com/media-p.slid.es/thumbnails/16be61fd6c4b16bbaf5a1fde6749a30f/thumb.jpg?1566314467","view_count":0,"user":{"id":1047136,"username":"lwalstad-pivotal","name":null,"description":null,"thumbnail_url":"https://www.gravatar.com/avatar/8e257aa94b14c5462bb5651052f8e1c3?s=140\u0026d=https%3A%2F%2Fstatic.slid.es%2Fimages%2Fdefault-profile-picture.png","paid":true,"pro":false,"lite":true,"team_id":null,"settings":{"id":3098317,"present_controls":true,"present_upsizing":true,"present_pointer":false,"present_notes":true,"default_deck_tag_id":null}},"background_transition":"none","transition":"slide","theme_id":null,"theme_font":"montserrat","theme_color":"white-blue","auto_slide_interval":0,"comments_enabled":true,"forking_enabled":false,"rolling_links":false,"center":false,"shuffle":false,"should_loop":false,"share_notes":true,"slide_number":true,"slide_count":18,"rtl":false,"version":2,"collaborative":null,"deck_user_editor_limit":1,"data_updated_at":1566315117325,"font_typekit":null,"font_google":null,"time_limit":null,"upsizing_enabled":true,"notes":{"99022204a578adbc418d74197c3120ec":"","ae546e6021aec2041ef2641f0cfe5292":"But first of all, for people not yet familiar with Pivotal Greenplum I would like to give a short introduction.\n \n… in 2015\n\nPivotal Greenplum delivers unmatched analytical query performance on massive volumes of data.\n\nThe Data Platform based on Pivotal Greenplum can be used by Business Analysts running BI reports, Data Scientists working with ML and AI algorithms and software developers working with custom applications. Greenplum is powered by an advanced cost-based optimizer, supports standard ANSI SQL interface, provides connectivity via ODBC / JDBC and includes libraries to work with large text, ML, programming languages such as R and Python and geo-spatial capabilities. It can manage multi-structured data, both internal and external, provides tools for integrating with external systems and can be deployed both on-premise and on variety of cloud platforms.\n","ebab84e1844796e4146cb89d057c3523":"We are saying: “Data tells the story”, it plays a central role in modern enterprises -- data must be captured, stored, processed and analyzed quickly for a company to be successful and competitive.\n\nBut data is not uniform - it can be structured (such as data in database tables), \nsemi-structured (such as CSV, JSON or XML files) or unstructured. \n\nEach data format requires customized processing logic and achieving an overlapping view of data becomes more and more difficult.\n","8cff6eb731897a85114cf1b366ae5824":"The complexity is further increased by the fact that data of all possible formats can be stored in multiple systems based on operational requirements. These include analytical engines such as Pivotal Greenplum, cloud storage providers, such as Amazon S3, on-premise RDMS, in-memory grids, such as Pivotal GemFire, processing systems such as Apache Spark and Kafka and finally data lakes based on Apache Hadoop technology. \n\nThe data gets fragmented across multiple systems with different APIs and storage formats, making analytics across them a daunting task.\n","45afadb5ca148c10bfed7dce92e70d12":"The complexity is further increased by the fact that data of all possible formats can be stored in multiple systems based on operational requirements. These include analytical engines such as Pivotal Greenplum, cloud storage providers, such as Amazon S3, on-premise RDMS, in-memory grids, such as Pivotal GemFire, processing systems such as Apache Spark and Kafka and finally data lakes based on Apache Hadoop technology. \n\nThe data gets fragmented across multiple systems with different APIs and storage formats, making analytics across them a daunting task.\n","4599448d37aafa870c97fb7044eea99d":"The main feature of Pivotal Greenplum that enables access to external data is that of External Table.\n\nExternal Table defines … (go over slide)\n\nI should mention that with the help of external tables a user can not only read data from other systems, but also write data to them. It is also possible to get data from a remote HTTP server by using WEB external tables, but these are beyond the scope of my presentation.\n\nAt run-time, when a query executor encounters the need to fetch data from an external table, it finds a plug-in connector for a protocol specified by the table and delegates the further processing to it. We’ll call this connector “a protocol handler”.\n","3e8d0a65f010c952e92ece2b88605532":"The protocol handler is the logic that … (go over slide)\n\nHere on the slide is the list of protocols for external access that Pivotal Greenplum supports. Traditionally these were oriented towards accessing files that reside either on the local file system of the Greenplum Segments, or on remote hosts or in the cloud. \n\nThere is also a protocol called “gphdfs” for accessing files stored in Hadoop HDFS. It works, but its operational characteristics and architecture could be improved. In addition, new protocol handlers need to be implemented in C programming language, which poses problems for systems that have Java APIs.\n\nSo the next-generation protocol was born to allow integration with wide varieties of systems, starting with Java-based Hadoop eco-system and extending to technologies such as JDBC and memory grids. It also makes integrating with a new system much easier by taking care of low-level Greenplum integration details and letting developers to focus to interactions with the external system by providing Java-based adapters. We call this protocol PXF.\n\n","8fb8479ef04d73d448b78bb37736f3e0":"The Greenplum Platform Extension Framework\n(PXF) provides parallel, high throughput data\naccess and federated queries across\nheterogeneous data sources. This provides the\nability to manage data on external partitions as if\nthe data was local. That means Greenplum can\nread, write, and query data from anywhere, even\nAmazon S3 or Hadoop data lakes.\n\n","5b64c01e6059dd65c69e8810f31f9e30":"Let’s consider the most popular use case of reading data from Hadoop HDFS into Greenplum:\nExternal table with PXF protocol location would be defined\nA SELECT query from this table is submitted to the Master\nMaster parses the query and instructs the segments to fetch data\nAll segment load the data in parallel, and each segment gets a thread in the PXF JVM\nPXF asks Namanode for the list of file fragments and divides the workload among the segments\nEach PXF thread reads its data using Hadoop HDFS Java APIs\nResults are sent back to the PXF extension on the segment and then onto the Master\nCombined results are presented to the client issuing the query\n\n","3ea7fdf503cd249eb4a3c0ac969ccb5c":"Let’s look in more details into the PXF components that are needed to implement the flow we just reviewed.\n\nThe first one is -- Fragmenter -- its responsibility is to take the external data and break it into independent pieces called fragments such that they can be read in parallel by different PXF threads working on a query.\n","54e151cd4a6bc0ff854f8236b60adcb0":"The second component is the Accessor. Its responsibility is to read the fragment and break it into a set of rows that would later be transformed into external table tuples.\n","48cf18c4f7c3d41109bacf9b4b5599d1":"The third and final component is the Resolver, which takes a row, breaks it into a set of fields and their values and then transforms the values to types supported by Greenplum. For example, if text-based CSV file is to be sent to Greenplum in a binary format, number values need to be converted to integers before being placed into a byte stream.\n","738b238ee0153fb247f3c9e907212bd9":"When users define external tables based on PXF protocol, they need to specify which Fragmenter, Accessor and Resolver to use. For their convenience, we introduced a concept of a Profile, which is a name for a pre-defined combination of these 3 specific components. \n\nFor example, the HdfsTextSimple profile is an alias that lets PXF know which specific components to use to handle text files stored on Hadoop HDFS.\n","0e5fe2230b3716e582d93ddd5893754d":"Finally, we will need to create an external table so that we can query the data.\n","f3ddf7d191412b99207d9ef8d403a0c4":"In summary, we learned how 3 different functional components work together to break the data from the external system into a number of fragments and then rows and then column values and send it to Greenplum to represent as tuples for an external table, which is a definition and abstraction of the external data.\n\nJust a note, that GReenplum can also write to external tables, in this case the process is reversed, as data is sent from the Greenplum query executor to PXF, which processes the tuples, converts them to the target format and stores in the external system. There is no fragmenter involved in this flow as data is already fragmented by Greenplum segments.\n","473b817a84cae39ed577045bb9ad27c2":"Let’s quickly explore the connectors and profiles available with PXF. \n\nThe HDFS connector allows Greenplum and PXF to read data stored in Hadoop HDFS in 4 different formats.\n\nSimple text format with field delimiters, JSON format and optimized storage formats such as Parquet and Avro.\n","c191ffd2704703c21028ce3aba469e24":"The PXF HIve connector allows users to query data managed by Hive. \n\nIt supports simple file formats such as Text and Json as well as optimized ones such as ORC and Parquet.\n\nIt’s worth noting that when reading data PXF does not dispatch the query to Hive execution engine, but instead reads and processes the underlying files directly.\n\nThat allows us to avoid the overhead of Hive query execution engine, since query processing is happening in Greenplum and we only need to supply the data to it.\n","9e92bc3c259411cb0bcd43eb33d71e18":"There are other PXF connectors available or in development. \n\nA few of them were developed by members of the PXF community outside of Pivotal, which illustrates the power and scalability of the open-sourced development model.\n\nWe always welcome new contributions and I hope maybe some of you will be interested to develop a connector to your system of choice. \n","279bf5d611b67eb0271ac02268b27b19":""}}};


			// Use local fonts
			SLConfig.fonts_url = 'lib/fonts/';
		</script>

		<script src="lib/reveal.min.js"></script>
		<script src="lib/offline.js"></script>

		<!-- Initialize the presentation -->
		<script>
			Reveal.initialize({
				width: 960,
				height: 700,
				margin: 0.05,
				

				hash: true,
				controls: true,
				progress: true,
				mouseWheel: false,
				showNotes: true,
				slideNumber: true,

				autoSlide: 0,
				autoSlideStoppable: true,

				center: false,
				shuffle: false,
				loop: false,
				rtl: false,

				transition: "slide",
				backgroundTransition: "none",

				highlight: {
					escapeHTML: false
				},

				dependencies: [
					{ src: 'lib/reveal-plugins/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'lib/reveal-plugins/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'lib/reveal-plugins/highlight/highlight.js' },
					{ src: 'lib/reveal-plugins/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'lib/reveal-plugins/zoom/zoom.js', async: true }
				]
			});
		</script>

		

	</body>
</html>
